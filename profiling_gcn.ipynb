{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu111\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import torch.nn.functional as F\n",
    "from pygcn.gcnio.data import dataio\n",
    "from pygcn.gcnio.util import utils\n",
    "from pygcn.gcn3 import GCN\n",
    "import scipy.sparse\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glog as log\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "#from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print('cuda: %s' % cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True):\n",
    "    adj_full = scipy.sparse.load_npz('./{}/adj_full.npz'.format(prefix))\n",
    "    adj_train = scipy.sparse.load_npz('./{}/adj_train.npz'.format(prefix))\n",
    "    role = json.load(open('./{}/role.json'.format(prefix)))\n",
    "    feats = np.load('./{}/feats.npy'.format(prefix))\n",
    "    class_map = json.load(open('./{}/class_map.json'.format(prefix)))\n",
    "    class_map = {int(k):v for k,v in class_map.items()}\n",
    "    assert len(class_map) == feats.shape[0]\n",
    "    # ---- normalize feats ----\n",
    "    train_nodes = np.array(list(set(adj_train.nonzero()[0])))\n",
    "    train_feats = feats[train_nodes]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_feats)\n",
    "    feats = scaler.transform(feats)\n",
    "    # -------------------------\n",
    "    return adj_full, adj_train, feats, class_map, role\n",
    "\n",
    "\n",
    "def process_graph_data(adj_full, adj_train, feats, class_map, role, name):\n",
    "    \"\"\"\n",
    "    setup vertex property map for output classes, train/val/test masks, and feats\n",
    "    INPUT:\n",
    "        G           graph-tool graph, full graph including training,val,testing\n",
    "        feats       ndarray of shape |V|xf\n",
    "        class_map   dictionary {vertex_id: class_id}\n",
    "        val_nodes   index of validation nodes\n",
    "        test_nodes  index of testing nodes\n",
    "    OUTPUT:\n",
    "        G           graph-tool graph unchanged\n",
    "        role        array of size |V|, indicating 'train'/'val'/'test'\n",
    "        class_arr   array of |V|x|C|, converted by class_map\n",
    "        feats       array of features unchanged\n",
    "    \"\"\"\n",
    "    num_vertices = adj_full.shape[0]\n",
    "    if isinstance(list(class_map.values())[0],list):\n",
    "        print(\"labels are list\")\n",
    "        num_classes = len(list(class_map.values())[0])\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        p = 0;\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[p] = argmax(v)\n",
    "            p = p+1\n",
    "    else:\n",
    "        num_classes = max(class_map.values()) - min(class_map.values()) + 1\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[k] = v\n",
    "    if name=='flickr' or name=='reddit' or name=='ppi' or name=='amazon' or name=='yelp':\n",
    "        class_arr = np.squeeze(class_arr.astype(int))\n",
    "        \n",
    "    return adj_full, adj_train, feats, class_arr, role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon\n",
      "labels are list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 16:11:10.811665 17265 <ipython-input-4-7aa82be4e7b6>:39] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0320 16:11:10.812594 17265 <ipython-input-4-7aa82be4e7b6>:40] (1569960, 1569960)\n",
      "I0320 16:11:10.813243 17265 <ipython-input-4-7aa82be4e7b6>:41] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0320 16:11:10.813858 17265 <ipython-input-4-7aa82be4e7b6>:42] (1569960, 1569960)\n",
      "I0320 16:11:10.814454 17265 <ipython-input-4-7aa82be4e7b6>:43] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0320 16:11:10.815156 17265 <ipython-input-4-7aa82be4e7b6>:44] (1569960, 200)\n",
      "I0320 16:11:10.815732 17265 <ipython-input-4-7aa82be4e7b6>:45] <class 'numpy.ndarray'>\n",
      "I0320 16:11:10.816422 17265 <ipython-input-4-7aa82be4e7b6>:46] (1569960,)\n",
      "I0320 16:11:10.816998 17265 <ipython-input-4-7aa82be4e7b6>:47] <class 'numpy.int64'>\n",
      "I0320 16:11:10.817582 17265 <ipython-input-4-7aa82be4e7b6>:48] <class 'numpy.ndarray'>\n",
      "I0320 16:11:10.818152 17265 <ipython-input-4-7aa82be4e7b6>:49] (1255968,)\n",
      "I0320 16:11:10.818725 17265 <ipython-input-4-7aa82be4e7b6>:50] <class 'numpy.ndarray'>\n",
      "I0320 16:11:10.819293 17265 <ipython-input-4-7aa82be4e7b6>:51] (78498,)\n",
      "I0320 16:11:10.819972 17265 <ipython-input-4-7aa82be4e7b6>:52] <class 'numpy.ndarray'>\n",
      "I0320 16:11:10.820535 17265 <ipython-input-4-7aa82be4e7b6>:53] (235494,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nflickr: (89250,1)\\nppi:    (14755,121)\\nreddit: (232965,1)\\namazon: (1569960,107)\\nyelp:   (716847,100)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you use the same data splits as you generated attacks\n",
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# load original dataset (to get clean features and labels)\n",
    "SMALL = False\n",
    "if SMALL:\n",
    "    dataset = 'cora'\n",
    "    data = dataio.Dataset(root='/tmp/', name=dataset)\n",
    "    adj, features, labels = data.adj, data.features, data.labels\n",
    "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "    \n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "else:\n",
    "    data_prefix = './dataset/amazon'\n",
    "    temp_data = load_data(data_prefix)\n",
    "    data_list = data_prefix.split('/')\n",
    "    print(data_list[-1])\n",
    "    train_data = process_graph_data(*temp_data,data_list[-1])\n",
    "    adj,adj_train,features,labels,role = train_data\n",
    "    features = scipy.sparse.csr_matrix(features)\n",
    "    idx_train = np.array(role['tr'])\n",
    "    idx_val = np.array(role['va'])\n",
    "    idx_test = np.array(role['te'])\n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(adj_train))\n",
    "    log.info(adj_train.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(labels[0]))\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "\n",
    "'''\n",
    "flickr: (89250,1)\n",
    "ppi:    (14755,121)\n",
    "reddit: (232965,1)\n",
    "amazon: (1569960,107)\n",
    "yelp:   (716847,100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "37\n",
      "4\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n",
    "print(labels[1])\n",
    "print(labels[8])\n",
    "print(labels.max())\n",
    "\n",
    "model = GCN(nfeat=features.shape[1], nhid=32, nclass=labels.max()+1, device=device)\n",
    "  \n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform data to GPU device ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0320 16:13:38.254261 17265 save.py:8] amazon\n",
      "I0320 16:13:38.255182 17265 save.py:9] <class 'torch.Tensor'>\n",
      "I0320 16:13:38.255808 17265 save.py:10] torch.Size([1569960, 1569960])\n",
      "I0320 16:15:15.913398 17265 save.py:23] amazon\n",
      "I0320 16:15:15.914568 17265 save.py:24] <class 'numpy.ndarray'>\n",
      "I0320 16:15:15.915311 17265 save.py:25] 264339468\n",
      "I0320 16:15:15.915896 17265 save.py:26] 264339468\n",
      "I0320 16:15:15.916586 17265 save.py:27] 264339468\n",
      "I0320 16:15:19.076092 17265 save.py:37] coo->csr finished.\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "TRAIN = 1\n",
    "if TRAIN:\n",
    "    #with profile(activities=[\n",
    "    #    ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    #    with record_function(\"model_fit\"):\n",
    "    #        model.fit(features, adj, labels, idx_train, train_iters=200, verbose=True, name='ppi')\n",
    "    #        torch.save(model.state_dict(),'./model/gcn.pt')\n",
    "    #print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    #prof.export_chrome_trace(\"trace.json\")\n",
    "    model.fit(features, adj, labels, idx_train, train_iters=200, verbose=True, name='amazon')\n",
    "    #torch.save(model.state_dict(),'./model/gcn.pt')\n",
    "TEST = 0\n",
    "if TEST:\n",
    "    model.load_state_dict(torch.load('./model/gcn.pt'))\n",
    "    model.eval()\n",
    "    model.test(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
