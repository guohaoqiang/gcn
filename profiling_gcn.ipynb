{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import torch.nn.functional as F\n",
    "from pygcn.gcnio.data import dataio\n",
    "from pygcn.gcnio.util import utils\n",
    "from pygcn.gcn import GCN\n",
    "import scipy.sparse\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glog as log\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print('cuda: %s' % cuda)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True):\n",
    "    adj_full = scipy.sparse.load_npz('./{}/adj_full.npz'.format(prefix))\n",
    "    adj_train = scipy.sparse.load_npz('./{}/adj_train.npz'.format(prefix))\n",
    "    role = json.load(open('./{}/role.json'.format(prefix)))\n",
    "    feats = np.load('./{}/feats.npy'.format(prefix))\n",
    "    class_map = json.load(open('./{}/class_map.json'.format(prefix)))\n",
    "    class_map = {int(k):v for k,v in class_map.items()}\n",
    "    assert len(class_map) == feats.shape[0]\n",
    "    # ---- normalize feats ----\n",
    "    train_nodes = np.array(list(set(adj_train.nonzero()[0])))\n",
    "    train_feats = feats[train_nodes]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_feats)\n",
    "    feats = scaler.transform(feats)\n",
    "    # -------------------------\n",
    "    return adj_full, adj_train, feats, class_map, role\n",
    "\n",
    "\n",
    "def process_graph_data(adj_full, adj_train, feats, class_map, role, name):\n",
    "    \"\"\"\n",
    "    setup vertex property map for output classes, train/val/test masks, and feats\n",
    "    INPUT:\n",
    "        G           graph-tool graph, full graph including training,val,testing\n",
    "        feats       ndarray of shape |V|xf\n",
    "        class_map   dictionary {vertex_id: class_id}\n",
    "        val_nodes   index of validation nodes\n",
    "        test_nodes  index of testing nodes\n",
    "    OUTPUT:\n",
    "        G           graph-tool graph unchanged\n",
    "        role        array of size |V|, indicating 'train'/'val'/'test'\n",
    "        class_arr   array of |V|x|C|, converted by class_map\n",
    "        feats       array of features unchanged\n",
    "    \"\"\"\n",
    "    num_vertices = adj_full.shape[0]\n",
    "    if isinstance(list(class_map.values())[0],list):\n",
    "        print(\"labels are list\")\n",
    "        num_classes = len(list(class_map.values())[0])\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        p = 0;\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[p] = argmax(v)\n",
    "            p = p+1\n",
    "    else:\n",
    "        num_classes = max(class_map.values()) - min(class_map.values()) + 1\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[k] = v\n",
    "    if name=='flickr' or name=='reddit' or name=='ppi' or name=='amazon' or name=='yelp':\n",
    "        class_arr = np.squeeze(class_arr.astype(int))\n",
    "        \n",
    "    return adj_full, adj_train, feats, class_arr, role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon\n",
      "labels are list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0228 22:34:35.626614 24971 <ipython-input-36-ed1d41fec3fb>:39] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0228 22:34:35.627515 24971 <ipython-input-36-ed1d41fec3fb>:40] (1569960, 1569960)\n",
      "I0228 22:34:35.628129 24971 <ipython-input-36-ed1d41fec3fb>:41] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0228 22:34:35.628807 24971 <ipython-input-36-ed1d41fec3fb>:42] (1569960, 1569960)\n",
      "I0228 22:34:35.629362 24971 <ipython-input-36-ed1d41fec3fb>:43] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0228 22:34:35.629944 24971 <ipython-input-36-ed1d41fec3fb>:44] (1569960, 200)\n",
      "I0228 22:34:35.630615 24971 <ipython-input-36-ed1d41fec3fb>:45] <class 'numpy.ndarray'>\n",
      "I0228 22:34:35.631145 24971 <ipython-input-36-ed1d41fec3fb>:46] (1569960,)\n",
      "I0228 22:34:35.631695 24971 <ipython-input-36-ed1d41fec3fb>:47] <class 'numpy.int64'>\n",
      "I0228 22:34:35.632252 24971 <ipython-input-36-ed1d41fec3fb>:48] <class 'numpy.ndarray'>\n",
      "I0228 22:34:35.632793 24971 <ipython-input-36-ed1d41fec3fb>:49] (1255968,)\n",
      "I0228 22:34:35.633334 24971 <ipython-input-36-ed1d41fec3fb>:50] <class 'numpy.ndarray'>\n",
      "I0228 22:34:35.633908 24971 <ipython-input-36-ed1d41fec3fb>:51] (78498,)\n",
      "I0228 22:34:35.634455 24971 <ipython-input-36-ed1d41fec3fb>:52] <class 'numpy.ndarray'>\n",
      "I0228 22:34:35.635049 24971 <ipython-input-36-ed1d41fec3fb>:53] (235494,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nflickr: (89250,1)\\nppi:    (14755,121)\\nreddit: (232965,1)\\namazon: (1569960,107)\\nyelp:   (716847,100)\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you use the same data splits as you generated attacks\n",
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# load original dataset (to get clean features and labels)\n",
    "SMALL = False\n",
    "if SMALL:\n",
    "    dataset = 'polblogs'\n",
    "    data = dataio.Dataset(root='/tmp/', name=dataset)\n",
    "    adj, features, labels = data.adj, data.features, data.labels\n",
    "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "    \n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "else:\n",
    "    data_prefix = './dataset/amazon'\n",
    "    temp_data = load_data(data_prefix)\n",
    "    data_list = data_prefix.split('/')\n",
    "    print(data_list[-1])\n",
    "    train_data = process_graph_data(*temp_data,data_list[-1])\n",
    "    adj,adj_train,features,labels,role = train_data\n",
    "    features = scipy.sparse.csr_matrix(features)\n",
    "    idx_train = np.array(role['tr'])\n",
    "    idx_val = np.array(role['va'])\n",
    "    idx_test = np.array(role['te'])\n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(adj_train))\n",
    "    log.info(adj_train.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(labels[0]))\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "\n",
    "'''\n",
    "flickr: (89250,1)\n",
    "ppi:    (14755,121)\n",
    "reddit: (232965,1)\n",
    "amazon: (1569960,107)\n",
    "yelp:   (716847,100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "37\n",
      "53\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n",
    "print(labels[1])\n",
    "print(labels[7])\n",
    "print(labels.max())\n",
    "\n",
    "# Model and optimizer\n",
    "if len(labels.shape)>1:\n",
    "    model = GCN(nfeat=features.shape[1], nhid=32, nclass=len(labels[0]), device=device)\n",
    "else:\n",
    "    model = GCN(nfeat=features.shape[1], nhid=32, nclass=labels.max()+1, device=device)\n",
    "    \n",
    "    \n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "(3,)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1],[2],[3]])\n",
    "b = np.squeeze(a)\n",
    "print(b)\n",
    "print(b.shape)\n",
    "\n",
    "from numpy import argmax\n",
    "a = np.array([0,0,1])\n",
    "print(argmax(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 4.654569149017334\n",
      "Epoch 10, training loss: 3.999567985534668\n",
      "Epoch 20, training loss: 3.535738945007324\n",
      "Epoch 30, training loss: 3.279048442840576\n",
      "Epoch 40, training loss: 3.0889415740966797\n",
      "Epoch 50, training loss: 2.967660903930664\n",
      "Epoch 60, training loss: 2.90462589263916\n",
      "Epoch 70, training loss: 2.8706634044647217\n",
      "Epoch 80, training loss: 2.847438097000122\n",
      "Epoch 90, training loss: 2.8307056427001953\n",
      "Epoch 100, training loss: 2.8176076412200928\n",
      "Epoch 110, training loss: 2.805354356765747\n",
      "Epoch 120, training loss: 2.7962636947631836\n",
      "Epoch 130, training loss: 2.7868523597717285\n",
      "Epoch 140, training loss: 2.778909921646118\n",
      "Epoch 150, training loss: 2.770613193511963\n",
      "Epoch 160, training loss: 2.7654590606689453\n",
      "Epoch 170, training loss: 2.759206533432007\n",
      "Epoch 180, training loss: 2.7541463375091553\n",
      "Epoch 190, training loss: 2.7500600814819336\n",
      "Test set results: loss= 2.6280 accuracy= 0.3918\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.3918, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "model.fit(features, adj, labels, idx_train, train_iters=200, verbose=True)\n",
    "# # using validation to pick model\n",
    "# model.fit(features, perturbed_adj, labels, idx_train, idx_val, train_iters=200, verbose=True)\n",
    "model.eval()\n",
    "# You can use the inner function of model to test\n",
    "model.test(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
