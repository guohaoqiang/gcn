{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu111\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import torch.nn.functional as F\n",
    "from pygcn.gcnio.data import dataio\n",
    "from pygcn.gcnio.util import utils\n",
    "from pygcn.gcn3 import GCN\n",
    "import scipy.sparse\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glog as log\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "#from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print('cuda: %s' % cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True):\n",
    "    adj_full = scipy.sparse.load_npz('./{}/adj_full.npz'.format(prefix))\n",
    "    adj_train = scipy.sparse.load_npz('./{}/adj_train.npz'.format(prefix))\n",
    "    role = json.load(open('./{}/role.json'.format(prefix)))\n",
    "    feats = np.load('./{}/feats.npy'.format(prefix))\n",
    "    class_map = json.load(open('./{}/class_map.json'.format(prefix)))\n",
    "    class_map = {int(k):v for k,v in class_map.items()}\n",
    "    assert len(class_map) == feats.shape[0]\n",
    "    # ---- normalize feats ----\n",
    "    train_nodes = np.array(list(set(adj_train.nonzero()[0])))\n",
    "    train_feats = feats[train_nodes]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_feats)\n",
    "    feats = scaler.transform(feats)\n",
    "    # -------------------------\n",
    "    return adj_full, adj_train, feats, class_map, role\n",
    "\n",
    "\n",
    "def process_graph_data(adj_full, adj_train, feats, class_map, role, name):\n",
    "    \"\"\"\n",
    "    setup vertex property map for output classes, train/val/test masks, and feats\n",
    "    INPUT:\n",
    "        G           graph-tool graph, full graph including training,val,testing\n",
    "        feats       ndarray of shape |V|xf\n",
    "        class_map   dictionary {vertex_id: class_id}\n",
    "        val_nodes   index of validation nodes\n",
    "        test_nodes  index of testing nodes\n",
    "    OUTPUT:\n",
    "        G           graph-tool graph unchanged\n",
    "        role        array of size |V|, indicating 'train'/'val'/'test'\n",
    "        class_arr   array of |V|x|C|, converted by class_map\n",
    "        feats       array of features unchanged\n",
    "    \"\"\"\n",
    "    num_vertices = adj_full.shape[0]\n",
    "    if isinstance(list(class_map.values())[0],list):\n",
    "        print(\"labels are list\")\n",
    "        num_classes = len(list(class_map.values())[0])\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        p = 0;\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[p] = argmax(v)\n",
    "            p = p+1\n",
    "    else:\n",
    "        num_classes = max(class_map.values()) - min(class_map.values()) + 1\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[k] = v\n",
    "    if name=='flickr' or name=='reddit' or name=='ppi' or name=='amazon' or name=='yelp':\n",
    "        class_arr = np.squeeze(class_arr.astype(int))\n",
    "        \n",
    "    return adj_full, adj_train, feats, class_arr, role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reddit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0319 22:02:42.262606 4916 <ipython-input-28-5afab1ff2513>:39] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0319 22:02:42.263366 4916 <ipython-input-28-5afab1ff2513>:40] (232965, 232965)\n",
      "I0319 22:02:42.263874 4916 <ipython-input-28-5afab1ff2513>:41] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0319 22:02:42.264352 4916 <ipython-input-28-5afab1ff2513>:42] (232965, 232965)\n",
      "I0319 22:02:42.264815 4916 <ipython-input-28-5afab1ff2513>:43] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0319 22:02:42.265388 4916 <ipython-input-28-5afab1ff2513>:44] (232965, 602)\n",
      "I0319 22:02:42.265848 4916 <ipython-input-28-5afab1ff2513>:45] <class 'numpy.ndarray'>\n",
      "I0319 22:02:42.266308 4916 <ipython-input-28-5afab1ff2513>:46] (232965,)\n",
      "I0319 22:02:42.266760 4916 <ipython-input-28-5afab1ff2513>:47] <class 'numpy.int64'>\n",
      "I0319 22:02:42.267226 4916 <ipython-input-28-5afab1ff2513>:48] <class 'numpy.ndarray'>\n",
      "I0319 22:02:42.267676 4916 <ipython-input-28-5afab1ff2513>:49] (153932,)\n",
      "I0319 22:02:42.268214 4916 <ipython-input-28-5afab1ff2513>:50] <class 'numpy.ndarray'>\n",
      "I0319 22:02:42.268662 4916 <ipython-input-28-5afab1ff2513>:51] (23699,)\n",
      "I0319 22:02:42.269112 4916 <ipython-input-28-5afab1ff2513>:52] <class 'numpy.ndarray'>\n",
      "I0319 22:02:42.269572 4916 <ipython-input-28-5afab1ff2513>:53] (55334,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nflickr: (89250,1)\\nppi:    (14755,121)\\nreddit: (232965,1)\\namazon: (1569960,107)\\nyelp:   (716847,100)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you use the same data splits as you generated attacks\n",
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# load original dataset (to get clean features and labels)\n",
    "SMALL = False\n",
    "if SMALL:\n",
    "    dataset = 'cora'\n",
    "    data = dataio.Dataset(root='/tmp/', name=dataset)\n",
    "    adj, features, labels = data.adj, data.features, data.labels\n",
    "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "    \n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "else:\n",
    "    data_prefix = './dataset/reddit'\n",
    "    temp_data = load_data(data_prefix)\n",
    "    data_list = data_prefix.split('/')\n",
    "    print(data_list[-1])\n",
    "    train_data = process_graph_data(*temp_data,data_list[-1])\n",
    "    adj,adj_train,features,labels,role = train_data\n",
    "    features = scipy.sparse.csr_matrix(features)\n",
    "    idx_train = np.array(role['tr'])\n",
    "    idx_val = np.array(role['va'])\n",
    "    idx_test = np.array(role['te'])\n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(adj_train))\n",
    "    log.info(adj_train.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(labels[0]))\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "\n",
    "'''\n",
    "flickr: (89250,1)\n",
    "ppi:    (14755,121)\n",
    "reddit: (232965,1)\n",
    "amazon: (1569960,107)\n",
    "yelp:   (716847,100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "17\n",
      "38\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n",
    "print(labels[1])\n",
    "print(labels[8])\n",
    "print(labels.max())\n",
    "\n",
    "model = GCN(nfeat=features.shape[1], nhid=32, nclass=labels.max()+1, device=device)\n",
    "  \n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform data to GPU device ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0319 22:03:03.536324 4916 gcn3.py:181] torch.float32\n",
      "I0319 22:03:03.537249 4916 gcn3.py:182] torch.float32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_train_without_val\n",
      "Epoch 0, training loss: 3.7913706302642822\n",
      "Epoch 10, training loss: 1.6687209606170654\n",
      "Epoch 20, training loss: 0.8948564529418945\n",
      "Epoch 30, training loss: 0.6476398706436157\n",
      "Epoch 40, training loss: 0.5512953996658325\n",
      "Epoch 50, training loss: 0.49943703413009644\n",
      "Epoch 60, training loss: 0.4650373160839081\n",
      "Epoch 70, training loss: 0.44400444626808167\n",
      "Epoch 80, training loss: 0.4302442669868469\n",
      "Epoch 90, training loss: 0.41057413816452026\n",
      "Epoch 100, training loss: 0.4032043218612671\n",
      "Epoch 110, training loss: 0.3940287232398987\n",
      "Epoch 120, training loss: 0.38555556535720825\n",
      "Epoch 130, training loss: 0.3775011897087097\n",
      "Epoch 140, training loss: 0.3719058930873871\n",
      "Epoch 150, training loss: 0.3623087704181671\n",
      "Epoch 160, training loss: 0.35698258876800537\n",
      "Epoch 170, training loss: 0.35585424304008484\n",
      "Epoch 180, training loss: 0.34975022077560425\n",
      "Epoch 190, training loss: 0.3454720675945282\n",
      "Forward time: 154.8847s\n",
      "Layer1 time: 0.0199s\n",
      "Layer1 (AX)W time: 0.0103s\n",
      "Layer reLU time: 0.0051s\n",
      "Layer2 time: 114.3098s\n",
      "Layer2 AX time: 114.2792s\n",
      "Layer2 (AX)W time: 0.0122s\n",
      "Backward time: 8.0461s\n",
      "Test set results: loss= 0.2713 accuracy= 0.9463\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "TRAIN = 1\n",
    "if TRAIN:\n",
    "    #with profile(activities=[\n",
    "    #    ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    #    with record_function(\"model_fit\"):\n",
    "    #        model.fit(features, adj, labels, idx_train, train_iters=200, verbose=True, name='ppi')\n",
    "    #        torch.save(model.state_dict(),'./model/gcn.pt')\n",
    "    #print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    #prof.export_chrome_trace(\"trace.json\")\n",
    "    model.fit(features, adj, labels, idx_train, train_iters=200, verbose=True, name='reddit')\n",
    "    torch.save(model.state_dict(),'./model/gcn.pt')\n",
    "TEST = 1\n",
    "if TEST:\n",
    "    model.load_state_dict(torch.load('./model/gcn.pt'))\n",
    "    model.eval()\n",
    "    model.test(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
