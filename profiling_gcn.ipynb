{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import torch.nn.functional as F\n",
    "from pygcn.gcnio.data import dataio\n",
    "from pygcn.gcnio.util import utils\n",
    "from pygcn.gcn import GCN\n",
    "import scipy.sparse\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glog as log\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print('cuda: %s' % cuda)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True):\n",
    "    adj_full = scipy.sparse.load_npz('./{}/adj_full.npz'.format(prefix))\n",
    "    adj_train = scipy.sparse.load_npz('./{}/adj_train.npz'.format(prefix))\n",
    "    role = json.load(open('./{}/role.json'.format(prefix)))\n",
    "    feats = np.load('./{}/feats.npy'.format(prefix))\n",
    "    class_map = json.load(open('./{}/class_map.json'.format(prefix)))\n",
    "    class_map = {int(k):v for k,v in class_map.items()}\n",
    "    assert len(class_map) == feats.shape[0]\n",
    "    # ---- normalize feats ----\n",
    "    train_nodes = np.array(list(set(adj_train.nonzero()[0])))\n",
    "    train_feats = feats[train_nodes]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_feats)\n",
    "    feats = scaler.transform(feats)\n",
    "    # -------------------------\n",
    "    return adj_full, adj_train, feats, class_map, role\n",
    "\n",
    "\n",
    "def process_graph_data(adj_full, adj_train, feats, class_map, role, name):\n",
    "    \"\"\"\n",
    "    setup vertex property map for output classes, train/val/test masks, and feats\n",
    "    INPUT:\n",
    "        G           graph-tool graph, full graph including training,val,testing\n",
    "        feats       ndarray of shape |V|xf\n",
    "        class_map   dictionary {vertex_id: class_id}\n",
    "        val_nodes   index of validation nodes\n",
    "        test_nodes  index of testing nodes\n",
    "    OUTPUT:\n",
    "        G           graph-tool graph unchanged\n",
    "        role        array of size |V|, indicating 'train'/'val'/'test'\n",
    "        class_arr   array of |V|x|C|, converted by class_map\n",
    "        feats       array of features unchanged\n",
    "    \"\"\"\n",
    "    num_vertices = adj_full.shape[0]\n",
    "    if isinstance(list(class_map.values())[0],list):\n",
    "        print(\"labels are list\")\n",
    "        num_classes = len(list(class_map.values())[0])\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        p = 0;\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[p] = argmax(v)\n",
    "            p = p+1\n",
    "    else:\n",
    "        num_classes = max(class_map.values()) - min(class_map.values()) + 1\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[k] = v\n",
    "    if name=='flickr' or name=='reddit' or name=='ppi' or name=='amazon' or name=='yelp':\n",
    "        class_arr = np.squeeze(class_arr.astype(int))\n",
    "        \n",
    "    return adj_full, adj_train, feats, class_arr, role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0301 17:18:27.463778 23504 <ipython-input-28-4ed42d5f1dc1>:39] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0301 17:18:27.464626 23504 <ipython-input-28-4ed42d5f1dc1>:40] (14755, 14755)\n",
      "I0301 17:18:27.465300 23504 <ipython-input-28-4ed42d5f1dc1>:41] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0301 17:18:27.465956 23504 <ipython-input-28-4ed42d5f1dc1>:42] (14755, 14755)\n",
      "I0301 17:18:27.466576 23504 <ipython-input-28-4ed42d5f1dc1>:43] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0301 17:18:27.467193 23504 <ipython-input-28-4ed42d5f1dc1>:44] (14755, 50)\n",
      "I0301 17:18:27.467799 23504 <ipython-input-28-4ed42d5f1dc1>:45] <class 'numpy.ndarray'>\n",
      "I0301 17:18:27.468408 23504 <ipython-input-28-4ed42d5f1dc1>:46] (14755,)\n",
      "I0301 17:18:27.469010 23504 <ipython-input-28-4ed42d5f1dc1>:47] <class 'numpy.int64'>\n",
      "I0301 17:18:27.469636 23504 <ipython-input-28-4ed42d5f1dc1>:48] <class 'numpy.ndarray'>\n",
      "I0301 17:18:27.470242 23504 <ipython-input-28-4ed42d5f1dc1>:49] (9716,)\n",
      "I0301 17:18:27.470847 23504 <ipython-input-28-4ed42d5f1dc1>:50] <class 'numpy.ndarray'>\n",
      "I0301 17:18:27.471445 23504 <ipython-input-28-4ed42d5f1dc1>:51] (1825,)\n",
      "I0301 17:18:27.472068 23504 <ipython-input-28-4ed42d5f1dc1>:52] <class 'numpy.ndarray'>\n",
      "I0301 17:18:27.472669 23504 <ipython-input-28-4ed42d5f1dc1>:53] (3214,)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppi\n",
      "labels are list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nflickr: (89250,1)\\nppi:    (14755,121)\\nreddit: (232965,1)\\namazon: (1569960,107)\\nyelp:   (716847,100)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you use the same data splits as you generated attacks\n",
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# load original dataset (to get clean features and labels)\n",
    "SMALL = False\n",
    "if SMALL:\n",
    "    dataset = 'polblogs'\n",
    "    data = dataio.Dataset(root='/tmp/', name=dataset)\n",
    "    adj, features, labels = data.adj, data.features, data.labels\n",
    "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "    \n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "else:\n",
    "    data_prefix = './dataset/ppi'\n",
    "    temp_data = load_data(data_prefix)\n",
    "    data_list = data_prefix.split('/')\n",
    "    print(data_list[-1])\n",
    "    train_data = process_graph_data(*temp_data,data_list[-1])\n",
    "    adj,adj_train,features,labels,role = train_data\n",
    "    features = scipy.sparse.csr_matrix(features)\n",
    "    idx_train = np.array(role['tr'])\n",
    "    idx_val = np.array(role['va'])\n",
    "    idx_test = np.array(role['te'])\n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(adj_train))\n",
    "    log.info(adj_train.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(labels[0]))\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "\n",
    "'''\n",
    "flickr: (89250,1)\n",
    "ppi:    (14755,121)\n",
    "reddit: (232965,1)\n",
    "amazon: (1569960,107)\n",
    "yelp:   (716847,100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "28\n",
      "0\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n",
    "print(labels[1])\n",
    "print(labels[8])\n",
    "print(labels.max())\n",
    "\n",
    "model = GCN(nfeat=features.shape[1], nhid=32, nclass=labels.max()+1, device=device)\n",
    "  \n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_train_without_val\n",
      "Epoch 0, training loss: 4.736241340637207\n",
      "Epoch 10, training loss: 3.9234836101531982\n",
      "Epoch 20, training loss: 2.5192980766296387\n",
      "Epoch 30, training loss: 1.6430875062942505\n",
      "Epoch 40, training loss: 1.50680673122406\n",
      "Epoch 50, training loss: 1.4283138513565063\n",
      "Epoch 60, training loss: 1.393007755279541\n",
      "Epoch 70, training loss: 1.379052996635437\n",
      "Epoch 80, training loss: 1.369570255279541\n",
      "Epoch 90, training loss: 1.3551537990570068\n",
      "Epoch 100, training loss: 1.3473656177520752\n",
      "Epoch 110, training loss: 1.3355250358581543\n",
      "Epoch 120, training loss: 1.3279798030853271\n",
      "Epoch 130, training loss: 1.3182791471481323\n",
      "Epoch 140, training loss: 1.3126503229141235\n",
      "Epoch 150, training loss: 1.3059924840927124\n",
      "Epoch 160, training loss: 1.3001426458358765\n",
      "Epoch 170, training loss: 1.2979130744934082\n",
      "Epoch 180, training loss: 1.293259859085083\n",
      "Epoch 190, training loss: 1.2910829782485962\n",
      "Forward time: 0.9983s\n",
      "Layer1 time: 0.5502s\n",
      "Layer1 XW time: 0.2841s\n",
      "Layer1 A(XW) time: 0.2414s\n",
      "Layer2 time: 0.2342s\n",
      "Layer2 XW time: 0.0097s\n",
      "Layer2 A(XW) time: 0.1977s\n",
      "Backward time: 1.2227s\n",
      "Test set results: loss= 1.2765 accuracy= 0.6764\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "TRAIN = 1\n",
    "if TRAIN:\n",
    "    model.fit(features, adj, labels, idx_train, train_iters=200, verbose=True, name='ppi')\n",
    "    torch.save(model.state_dict(),'./model/gcn.pt')\n",
    "TEST = 1\n",
    "if TEST:\n",
    "    model.load_state_dict(torch.load('./model/gcn.pt'))\n",
    "    model.eval()\n",
    "    model.test(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
