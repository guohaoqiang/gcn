{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.1+cu111\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "from numpy import argmax\n",
    "import torch.nn.functional as F\n",
    "from pygcn.gcnio.data import dataio\n",
    "from pygcn.gcnio.util import utils\n",
    "from pygcn.gcn3 import GCN\n",
    "import scipy.sparse\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import glog as log\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda: True\n"
     ]
    }
   ],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "print('cuda: %s' % cuda)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(prefix, normalize=True):\n",
    "    adj_full = scipy.sparse.load_npz('./{}/adj_full.npz'.format(prefix))\n",
    "    adj_train = scipy.sparse.load_npz('./{}/adj_train.npz'.format(prefix))\n",
    "    role = json.load(open('./{}/role.json'.format(prefix)))\n",
    "    feats = np.load('./{}/feats.npy'.format(prefix))\n",
    "    class_map = json.load(open('./{}/class_map.json'.format(prefix)))\n",
    "    class_map = {int(k):v for k,v in class_map.items()}\n",
    "    assert len(class_map) == feats.shape[0]\n",
    "    # ---- normalize feats ----\n",
    "    train_nodes = np.array(list(set(adj_train.nonzero()[0])))\n",
    "    train_feats = feats[train_nodes]\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_feats)\n",
    "    feats = scaler.transform(feats)\n",
    "    # -------------------------\n",
    "    return adj_full, adj_train, feats, class_map, role\n",
    "\n",
    "\n",
    "def process_graph_data(adj_full, adj_train, feats, class_map, role, name):\n",
    "    \"\"\"\n",
    "    setup vertex property map for output classes, train/val/test masks, and feats\n",
    "    INPUT:\n",
    "        G           graph-tool graph, full graph including training,val,testing\n",
    "        feats       ndarray of shape |V|xf\n",
    "        class_map   dictionary {vertex_id: class_id}\n",
    "        val_nodes   index of validation nodes\n",
    "        test_nodes  index of testing nodes\n",
    "    OUTPUT:\n",
    "        G           graph-tool graph unchanged\n",
    "        role        array of size |V|, indicating 'train'/'val'/'test'\n",
    "        class_arr   array of |V|x|C|, converted by class_map\n",
    "        feats       array of features unchanged\n",
    "    \"\"\"\n",
    "    num_vertices = adj_full.shape[0]\n",
    "    if isinstance(list(class_map.values())[0],list):\n",
    "        print(\"labels are list\")\n",
    "        num_classes = len(list(class_map.values())[0])\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        p = 0;\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[p] = argmax(v)\n",
    "            p = p+1\n",
    "    else:\n",
    "        num_classes = max(class_map.values()) - min(class_map.values()) + 1\n",
    "        class_arr = np.zeros((num_vertices, 1))\n",
    "        for k,v in class_map.items():\n",
    "            class_arr[k] = v\n",
    "    if name=='flickr' or name=='reddit' or name=='ppi' or name=='amazon' or name=='yelp':\n",
    "        class_arr = np.squeeze(class_arr.astype(int))\n",
    "        \n",
    "    return adj_full, adj_train, feats, class_arr, role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon\n",
      "labels are list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0315 18:18:03.031167 15584 <ipython-input-28-a1c67f2afaeb>:39] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0315 18:18:03.032286 15584 <ipython-input-28-a1c67f2afaeb>:40] (1569960, 1569960)\n",
      "I0315 18:18:03.032942 15584 <ipython-input-28-a1c67f2afaeb>:41] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0315 18:18:03.033547 15584 <ipython-input-28-a1c67f2afaeb>:42] (1569960, 1569960)\n",
      "I0315 18:18:03.034124 15584 <ipython-input-28-a1c67f2afaeb>:43] <class 'scipy.sparse.csr.csr_matrix'>\n",
      "I0315 18:18:03.034687 15584 <ipython-input-28-a1c67f2afaeb>:44] (1569960, 200)\n",
      "I0315 18:18:03.035298 15584 <ipython-input-28-a1c67f2afaeb>:45] <class 'numpy.ndarray'>\n",
      "I0315 18:18:03.035863 15584 <ipython-input-28-a1c67f2afaeb>:46] (1569960,)\n",
      "I0315 18:18:03.036429 15584 <ipython-input-28-a1c67f2afaeb>:47] <class 'numpy.int64'>\n",
      "I0315 18:18:03.036979 15584 <ipython-input-28-a1c67f2afaeb>:48] <class 'numpy.ndarray'>\n",
      "I0315 18:18:03.037560 15584 <ipython-input-28-a1c67f2afaeb>:49] (1255968,)\n",
      "I0315 18:18:03.038116 15584 <ipython-input-28-a1c67f2afaeb>:50] <class 'numpy.ndarray'>\n",
      "I0315 18:18:03.038671 15584 <ipython-input-28-a1c67f2afaeb>:51] (78498,)\n",
      "I0315 18:18:03.039217 15584 <ipython-input-28-a1c67f2afaeb>:52] <class 'numpy.ndarray'>\n",
      "I0315 18:18:03.039772 15584 <ipython-input-28-a1c67f2afaeb>:53] (235494,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nflickr: (89250,1)\\nppi:    (14755,121)\\nreddit: (232965,1)\\namazon: (1569960,107)\\nyelp:   (716847,100)\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make sure you use the same data splits as you generated attacks\n",
    "seed = 15\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if cuda:\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# load original dataset (to get clean features and labels)\n",
    "SMALL = False\n",
    "if SMALL:\n",
    "    dataset = 'pubmed'\n",
    "    data = dataio.Dataset(root='/tmp/', name=dataset)\n",
    "    adj, features, labels = data.adj, data.features, data.labels\n",
    "    idx_train, idx_val, idx_test = data.idx_train, data.idx_val, data.idx_test\n",
    "    \n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "else:\n",
    "    data_prefix = './dataset/amazon'\n",
    "    temp_data = load_data(data_prefix)\n",
    "    data_list = data_prefix.split('/')\n",
    "    print(data_list[-1])\n",
    "    train_data = process_graph_data(*temp_data,data_list[-1])\n",
    "    adj,adj_train,features,labels,role = train_data\n",
    "    features = scipy.sparse.csr_matrix(features)\n",
    "    idx_train = np.array(role['tr'])\n",
    "    idx_val = np.array(role['va'])\n",
    "    idx_test = np.array(role['te'])\n",
    "    log.info(type(adj))\n",
    "    log.info(adj.shape)\n",
    "    log.info(type(adj_train))\n",
    "    log.info(adj_train.shape)\n",
    "    log.info(type(features))\n",
    "    log.info(features.shape)\n",
    "    log.info(type(labels))\n",
    "    log.info(labels.shape)\n",
    "    log.info(type(labels[0]))\n",
    "    log.info(type(idx_train))\n",
    "    log.info(idx_train.shape)\n",
    "    log.info(type(idx_val))\n",
    "    log.info(idx_val.shape)\n",
    "    log.info(type(idx_test))\n",
    "    log.info(idx_test.shape)\n",
    "\n",
    "'''\n",
    "flickr: (89250,1)\n",
    "ppi:    (14755,121)\n",
    "reddit: (232965,1)\n",
    "amazon: (1569960,107)\n",
    "yelp:   (716847,100)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "37\n",
      "4\n",
      "101\n"
     ]
    }
   ],
   "source": [
    "print(labels[0])\n",
    "print(labels[1])\n",
    "print(labels[8])\n",
    "print(labels.max())\n",
    "\n",
    "model = GCN(nfeat=features.shape[1], nhid=32, nclass=labels.max()+1, device=device)\n",
    "  \n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=0.01, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform data to GPU device ...\n",
      "_train_without_val\n",
      "Epoch 0, training loss: 4.654569149017334\n",
      "Epoch 10, training loss: 3.999568462371826\n",
      "Epoch 20, training loss: 3.5357396602630615\n",
      "Epoch 30, training loss: 3.279048204421997\n",
      "Epoch 40, training loss: 3.0889408588409424\n",
      "Epoch 50, training loss: 2.9676637649536133\n",
      "Epoch 60, training loss: 2.904623508453369\n",
      "Epoch 70, training loss: 2.870666265487671\n",
      "Epoch 80, training loss: 2.8474409580230713\n",
      "Epoch 90, training loss: 2.830698013305664\n",
      "Epoch 100, training loss: 2.817601203918457\n",
      "Epoch 110, training loss: 2.8053479194641113\n",
      "Epoch 120, training loss: 2.7962639331817627\n",
      "Epoch 130, training loss: 2.7868757247924805\n",
      "Epoch 140, training loss: 2.778928279876709\n",
      "Epoch 150, training loss: 2.7706377506256104\n",
      "Epoch 160, training loss: 2.7654449939727783\n",
      "Epoch 170, training loss: 2.7592170238494873\n",
      "Epoch 180, training loss: 2.7541704177856445\n",
      "Epoch 190, training loss: 2.7500991821289062\n",
      "Forward time: 618.6523s\n",
      "Layer1 time: 0.0324s\n",
      "Layer1 (AX)W time: 0.0180s\n",
      "Layer reLU time: 0.0062s\n",
      "Layer2 time: 397.5692s\n",
      "Layer2 AX time: 397.5241s\n",
      "Layer2 (AX)W time: 0.0225s\n",
      "Backward time: 99.8559s\n",
      "Test set results: loss= 2.6280 accuracy= 0.3919\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "TRAIN = 1\n",
    "if TRAIN:\n",
    "    #with profile(activities=[\n",
    "    #    ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    #    with record_function(\"model_fit\"):\n",
    "    #        model.fit(features, adj, labels, idx_train, train_iters=200, verbose=True, name='ppi')\n",
    "    #        torch.save(model.state_dict(),'./model/gcn.pt')\n",
    "    #print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "    #prof.export_chrome_trace(\"trace.json\")\n",
    "    model.fit(features, adj, labels, idx_train, train_iters=200, verbose=True, name='amazon')\n",
    "    torch.save(model.state_dict(),'./model/gcn.pt')\n",
    "TEST = 1\n",
    "if TEST:\n",
    "    model.load_state_dict(torch.load('./model/gcn.pt'))\n",
    "    model.eval()\n",
    "    model.test(idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
